# Blueprint Cluster 1 (Cluster A)
# ======================================================
# Questo blueprint definisce il primo cluster del Cloud Continuum.
# Comprende tre VM:
# - fedcontr_1 : nodo controller (Keycloak + CouchDB)
# - node2_1 : primo nodo worker con K3s e Crossplane
# - node3_1 : secondo nodo worker con K3s e Crossplane
# La rete Ã¨ unica (mgmt) e si trova nella subnet 192.168.1.0/24.
# ======================================================

ExperimentName: exp1a
Description: "Cloud Continuum Blueprint - Cluster 1"
Version: 1.0
Author: 
  name: Federico Balistreri
  email: federico.balistreri@studio.unibo.it
Date: 2025-10-26

# Rete di management usata da tutte le VM del cluster
Networks:
  - name: mgmt
    subnet: 192.168.1.0/24
    gateway: 192.168.1.1

Resources:
  # Nodo controller del cluster
  - name: fedcontr_1
    type: vm
    properties:
      provider: proxmox 
      cpu: 2
      memory: 4192
      network: mgmt
      ip: 192.168.1.100    # IP del nodo controller

    role: fedcontr
    services:
      platform: []         # Nessun servizio platform (docker/k8s) sul controller
      application:
        # Identity provider del cluster
        - name: keycloak   
          port: 8443
          image: quay.io/keycloak/keycloak:26.0.7
          env:
            KC_BOOTSTRAP_ADMIN_USERNAME: admin
            KC_BOOTSTRAP_ADMIN_PASSWORD: admin
          files:
            - source: keycloak.key
              destination: /cert/key.pem
              type: secret
            - source: keycloak.crt
              destination: /cert/cert.pem
            - source: realm.json
              destination: /import/realm.json
        # Database distribuito tra i cluster
        - name: couchdb
          port: 5984
          image: couchdb:3.4.2
          env:
            COUCHDB_USER: admin
            COUCHDB_PASSWORD: password
          init:
            create_dbs:
              - name: experiments
                documents:
                  # Entry iniziale del database
                  - id: experiment1
                    ExperimentName: exp1a
                    ExperimentGroup: experiment1
                    EndTime: "1/1/1"
                    SiteID: bologna1
                    ResourceKind: xslicenamespaces.experiment.mmwunibo.it
                    ResourceObject:
                      virtualMemory: 4Gb
                      virtualCpu: 2
                      experimentGroup: experiment1
  # Primo worker del cluster
  - name: node2_1
    depends_on: [fedcontr_1]  # Il worker si crea solo dopo il controller
    type: vm
    properties:
      provider: proxmox 
      cpu: 2
      memory: 4192
      network: mgmt
      ip: 192.168.1.101       # IP del primo worker

    role: node2
    services:
      platform:
        # Installazione di K3s (Kubernetes lightweight)
        - name: k3s
          args:
            - "--kube-apiserver-arg=oidc-ca-file=/home/vagrant/rootCA.crt"
            - "--kube-apiserver-arg=oidc-groups-claim=groups"
            - "--kube-apiserver-arg=oidc-issuer-url=https://192.168.1.100:8443/realms/kubernetes"
            - "--kube-apiserver-arg=oidc-client-id=bologna1"
            - "--kube-apiserver-arg=oidc-username-claim=sub"
            - "--bind-address=0.0.0.0"

        # Crossplane per provisioning Kubernetes e risorse cloud
        - name: crossplane
          namespace: crossplane-system
          helm_chart: crossplane-stable/crossplane

      # Componenti applicativi deployati sul cluster
      application:
        - name: experiment-provider
          deploy_path: ./deployments/experiment-provider
        - name: kubernetes-provider
          deploy_path: ./deployments/kubernetes-provider
        - name: couchDBsyncoperator
          deploy_path: ./deployments/couchDBsyncoperator
        - name: compositeResources
          deploy_path: ./deployments/compositeResources
  
  # Secondo worker del cluster
  - name: node3_1
    depends_on: [fedcontr_1]
    type: vm
    properties:
      provider: proxmox 
      cpu: 2
      memory: 4192
      network: mgmt
      ip: 192.168.1.102   # IP del secondo worker

    role: node3
    services:
      platform:
        # Identica configurazione OIDC del nodo2
        - name: k3s
          args:
            - "--kube-apiserver-arg=oidc-ca-file=/home/vagrant/rootCA.crt"
            - "--kube-apiserver-arg=oidc-groups-claim=groups"
            - "--kube-apiserver-arg=oidc-issuer-url=https://192.168.1.100:8443/realms/kubernetes"
            - "--kube-apiserver-arg=oidc-client-id=bologna1"
            - "--kube-apiserver-arg=oidc-username-claim=sub"
            - "--bind-address=0.0.0.0"

        - name: crossplane
          namespace: crossplane-system
          helm_chart: crossplane-stable/crossplane

      # Componenti identici al nodo2 
      application:
        - name: experiment-provider
          deploy_path: ./deployments/experiment-provider
        - name: kubernetes-provider
          deploy_path: ./deployments/kubernetes-provider
        - name: couchDBsyncoperator
          deploy_path: ./deployments/couchDBsyncoperator
        - name: compositeResources
          deploy_path: ./deployments/compositeResources

